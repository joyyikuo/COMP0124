{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Processing\n",
    "\n",
    "def CleanDF(df):\n",
    "\n",
    "    df[\"adexchange\"] = df['adexchange'].fillna(0)\n",
    "    df[\"usertag\"] = df['usertag'].fillna(\"\") \n",
    "    df['slotsize'] = df['slotwidth'].astype(str) + 'x' + df['slotheight'].astype(str)   \n",
    "\n",
    "    #remove ids from the dataset since they won't be too useful in the model\n",
    "    df.drop(['bidid', 'userid', 'domain', 'url', 'urlid', \n",
    "             'slotid', 'creative', \"slotwidth\", \"slotheight\"], axis = 1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2429188\n",
      "1       1793\n",
      "Name: click, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#only importing train and validation since test isn't useable \n",
    "traindf = pd.read_csv(\"train.csv\")\n",
    "validationdf = pd.read_csv(\"validation.csv\")\n",
    "testdf = pd.read_csv(\"test.csv\")\n",
    "\n",
    "traindf = CleanDF(traindf)\n",
    "validationdf = CleanDF(validationdf)\n",
    "testdf = CleanDF(testdf)\n",
    "\n",
    "traindf.head(5)\n",
    "\n",
    "print(traindf[\"click\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UsertagCategories(df):\n",
    "    \n",
    "    # Drop nan\n",
    "    df = df[\"usertag\"].dropna().reset_index(drop = True)\n",
    "    \n",
    "    # Find unique usertags\n",
    "    usertags_list = [df[i].split(\",\") for i in range(df.shape[0])]\n",
    "    \n",
    "    # itertools.chain.from_iterable joins a list of lists into a single list\n",
    "    usertags = np.unique(list(itertools.chain.from_iterable(usertags_list)))\n",
    "    \n",
    "    # Remove the empty string \"\"\n",
    "    usertags = [tag for tag in usertags if len(tag) > 0]\n",
    "    \n",
    "    return usertags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureEngineering(df): \n",
    "        \n",
    "        # Usertags\n",
    "        usertags = UsertagCategories(df)\n",
    "        for tag in usertags:\n",
    "            col_name = \"usertag_\" + tag\n",
    "            df[col_name] = df[\"usertag\"].map(lambda x: 1 if tag in x.split(\",\") else 0)\n",
    "            \n",
    "            \n",
    "        # num of usertags\n",
    "        df['usertagsCount'] = df.usertag.str.count(\",\")+1\n",
    "            \n",
    "        # Slotprice binning\n",
    "        df[\"slotprice_cat\"] = 0\n",
    "        \n",
    "        df.loc[ df[\"slotprice\"] <= 10, \"slotprice_cat\"] = 0\n",
    "        df.loc[ (df[\"slotprice\"] > 10) & (df[\"slotprice\"] <= 50), \"slotprice_cat\"] = 1\n",
    "        df.loc[ (df[\"slotprice\"] > 50) & (df[\"slotprice\"] <= 100), \"slotprice_cat\"] = 2\n",
    "        df.loc[ df[\"slotprice\"] > 100, \"slotprice_cat\"] = 3\n",
    "\n",
    "        \n",
    "        # IP Category\n",
    "        df['IP_cat'] = \"\"     \n",
    "        df[\"IP_str\"] = df[\"IP\"].astype(str)\n",
    "\n",
    "        df[\"IP_spt\"] = df[\"IP_str\"].map(lambda x: x.split(\".\")[0]).astype(int)\n",
    "\n",
    "        df.loc[ df[\"IP_spt\"] <= 127, \"IP_cat\"] = \"Class_A\"\n",
    "        df.loc[ (df[\"IP_spt\"] > 128) & (df[\"IP_spt\"] <= 191), \"IP_cat\"] = \"Class_B\"\n",
    "        df.loc[ (df[\"IP_spt\"] > 192) & (df[\"IP_spt\"] <= 223), \"IP_cat\"] = \"Class_C\"\n",
    "        df.loc[ df[\"IP_spt\"] >= 224, \"IP_cat\"] = \"Class_D\"         \n",
    "        \n",
    "             # Part of the day\n",
    "        df[\"part_of_day\"] = \"\"\n",
    "\n",
    "        df.loc[(df[\"weekday\"] == 0) & ((df[\"hour\"] < 8) & (df[\"hour\"] > 17)), \"part_of_day\"] = \"Sun_Night\"\n",
    "        df.loc[(df[\"weekday\"] == 0) & ((df[\"hour\"] >= 8) & (df[\"hour\"] <= 17)), \"part_of_day\"] = \"Sun_Morn\"\n",
    "\n",
    "        df.loc[(df[\"weekday\"] == 1) & ((df[\"hour\"] < 8) & (df[\"hour\"] > 17)), \"part_of_day\"] = \"Mon_Night\"\n",
    "        df.loc[(df[\"weekday\"] == 1) & ((df[\"hour\"] >= 8) & (df[\"hour\"] <= 17)), \"part_of_day\"] = \"Mon_Morn\"\n",
    "\n",
    "        df.loc[(df[\"weekday\"] == 2) & ((df[\"hour\"] < 8) & (df[\"hour\"] > 17)), \"part_of_day\"] = \"Tues_Night\"\n",
    "        df.loc[(df[\"weekday\"] == 2) & ((df[\"hour\"] >= 8) & (df[\"hour\"] <= 17)), \"part_of_day\"] = \"Tues_Morn\"\n",
    "\n",
    "        df.loc[(df[\"weekday\"] == 3) & ((df[\"hour\"] < 8) & (df[\"hour\"] > 17)), \"part_of_day\"] = \"Wed_Night\"\n",
    "        df.loc[(df[\"weekday\"] == 3) & ((df[\"hour\"] >= 8) & (df[\"hour\"] <= 17)), \"part_of_day\"] = \"Wed_Morn\"\n",
    "\n",
    "        df.loc[(df[\"weekday\"] == 4) & ((df[\"hour\"] < 8) & (df[\"hour\"] > 17)), \"part_of_day\"] = \"Thur_Night\"\n",
    "        df.loc[(df[\"weekday\"] == 4) & ((df[\"hour\"] >= 8) & (df[\"hour\"] <= 17)), \"part_of_day\"] = \"Thur_Morn\"\n",
    "\n",
    "        df.loc[(df[\"weekday\"] == 5) & ((df[\"hour\"] < 8) & (df[\"hour\"] > 17)), \"part_of_day\"] = \"Fri_Night\"\n",
    "        df.loc[(df[\"weekday\"] == 5) & ((df[\"hour\"] >= 8) & (df[\"hour\"] <= 17)), \"part_of_day\"] = \"Fri_Morn\"\n",
    "\n",
    "        df.loc[(df[\"weekday\"] == 6) & ((df[\"hour\"] < 8) & (df[\"hour\"] > 17)), \"part_of_day\"] = \"Sat_Night\"\n",
    "        df.loc[(df[\"weekday\"] == 6) & ((df[\"hour\"] >= 8) & (df[\"hour\"] <= 17)), \"part_of_day\"] = \"Sat_Morn\"\n",
    "        \n",
    "        # Convert numerical to categorical\n",
    "        df[\"weekday\"] = df[\"weekday\"].map(lambda x: str(x))\n",
    "        df[\"hour\"] = df[\"hour\"].map(lambda x: str(x))\n",
    "        df[\"region\"] = df[\"region\"].map(lambda x: str(x))\n",
    "        df[\"city\"] = df[\"city\"].map(lambda x: str(x))\n",
    "        df[\"adexchange\"] = df[\"adexchange\"].map(lambda x: str(x))\n",
    "        df[\"advertiser\"] = df[\"advertiser\"].map(lambda x: str(x))\n",
    "        \n",
    "        # Operating system\n",
    "        df[\"os\"] = df[\"useragent\"].map(lambda x: x.split(\"_\")[0])\n",
    "        \n",
    "        # Browser\n",
    "        df[\"browser\"] = df[\"useragent\"].map(lambda x: x.split(\"_\")[1])\n",
    "        \n",
    "        #Clean slotvisibility\n",
    "        #0\n",
    "        # FirstView\n",
    "        # 2\n",
    "        # 1\n",
    "        # OtherView\n",
    "        # SecondView\n",
    "        # Na\n",
    "        # 255\n",
    "        # ThirdView\n",
    "        # FifthView\n",
    "        # FourthView\n",
    "\n",
    "        \n",
    "        columns = [\"useragent\", \"slotprice\", \"usertag\"]\n",
    "        df.drop(columns, axis = 1, inplace = True)\n",
    "        \n",
    "        df = pd.get_dummies(df)\n",
    "  \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train feature\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9f58ecd80385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating train feature\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtraindf_features\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mFeatureEngineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraindf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating validation feature\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvalidationdf_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureEngineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidationdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating test feature\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-923e8a888dc3>\u001b[0m in \u001b[0;36mFeatureEngineering\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    890\u001b[0m             dummy = _get_dummies_1d(col[1], prefix=pre, prefix_sep=sep,\n\u001b[0;32m    891\u001b[0m                                     \u001b[0mdummy_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy_na\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                                     drop_first=drop_first, dtype=dtype)\n\u001b[0m\u001b[0;32m    893\u001b[0m             \u001b[0mwith_dummies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_dummies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[1;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 980\u001b[1;33m         \u001b[0mdummy_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdummy_na\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\twodim_base.py\u001b[0m in \u001b[0;36meye\u001b[1;34m(N, M, k, dtype, order)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mM\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Creating train feature\")\n",
    "traindf_features  = FeatureEngineering(traindf)\n",
    "print(\"Creating validation feature\")\n",
    "validationdf_features = FeatureEngineering(validationdf)\n",
    "print(\"Creating test feature\")\n",
    "testdf_features = FeatureEngineering(testdf)\n",
    "\n",
    "print(traindf_features.shape)\n",
    "traindf_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Because there is such a huge disparity between non-clicks vs clicks, we'll perform a:\n",
    "Negative downsampling (imbalanced): different samples sizes are used in this procedure. \n",
    "In all these samples all the observations from the minority class are kept and \n",
    "we take different number of observations from the majority class by performing sampling without replacement.\n",
    "'''\n",
    "\n",
    "\n",
    "# Separate majority and minority classes\n",
    "train_data_majority = traindf_features[traindf_features.click==0]\n",
    "train_data_minority = traindf_features[traindf_features.click==1]\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(train_data_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=20000,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "train_downsized = pd.concat([df_majority_downsampled, train_data_minority])\n",
    "\n",
    "\n",
    "print(\"Clicked data size ::::\" + str(train_data_minority.shape))\n",
    "print(\"Not clicked data size ::::\" + str(df_majority_downsampled.shape))\n",
    "\n",
    "# Display new class counts\n",
    "print(\"Combined dataset count ::::\" + str(train_downsized.click.value_counts()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = train_downsized.loc[:, ((train_downsized.columns != \"click\") & \n",
    "                      (train_downsized.columns != \"payprice\") &\n",
    "                      (train_downsized.columns != \"bidprice\"))]\n",
    "\n",
    "X_train_pay = train_downsized.loc[:, ((train_downsized.columns != \"click\") &\n",
    "                      (train_downsized.columns != \"bidprice\"))]\n",
    "\n",
    "\n",
    "y_train = train_downsized[\"click\"]\n",
    "\n",
    "X_validation = validationdf_features.loc[:, ((validationdf_features.columns != \"click\") & \n",
    "                                        (validationdf_features.columns != \"payprice\") & \n",
    "                                        (validationdf_features.columns != \"bidprice\"))]\n",
    "y_validation = validationdf_features[\"click\"]\n",
    "\n",
    "\n",
    "X_test = testdf_features.loc[:, ((testdf_features.columns != \"click\") & \n",
    "                                        (testdf_features.columns != \"payprice\") & \n",
    "                                        (testdf_features.columns != \"bidprice\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the down\n",
    "\n",
    "#Pickled Train\n",
    "pickle.dump(X_train, open(\"X_train_features.pkl\", 'wb'))\n",
    "pickle.dump(y_train, open(\"y_train_features.pkl\", 'wb'))\n",
    "\n",
    "pickle.dump(X_train_pay, open(\"X_train_pay.pkl\", 'wb'))\n",
    "\n",
    "#Pickled Validation\n",
    "pickle.dump(X_validation, open(\"X_validation_features.pkl\", 'wb'))\n",
    "pickle.dump(y_validation, open(\"y_validation_features.pkl\", 'wb'))\n",
    "\n",
    "#Pickled Test\n",
    "pickle.dump(X_test, open(\"X_test_features.pkl\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C = 0.1, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Recursive Feature Elimination object\n",
    "stepsize = 10\n",
    "rfecv = RFECV(estimator = clf, step = stepsize, cv = StratifiedKFold(n_splits = 2), scoring = \"f1_weighted\")\n",
    "\n",
    "rfecv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of all the features that our model with benefit form\n",
    "\n",
    "bestFeatures=  [feature for feature, booln in zip(list(X_train.columns.values), rfecv.support_) if booln]\n",
    "print(\"Selecting the \"+ str(len(bestFeatures)) + \" best features, which include\")\n",
    "print(bestFeatures, end=\", \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "#pickle.dump(rfecv, open(bestFeatures, 'wb'))\n",
    "\n",
    "\n",
    "with open('bestFeatures.pkl', 'wb') as f:\n",
    "    pickle.dump(bestFeatures, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
